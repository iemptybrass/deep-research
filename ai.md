
# Comparison of AI Coding Models (March 2025)

This report compares leading AI models fine-tuned for code generation across programming languages. Models are grouped into **open-source (on-device)** models (free to use locally) and **proprietary cloud** models (accessible via API or subscription). Key factors are **Pricing** (most important), supported languages/use cases, deployment options, model size/architecture, speed/latency, accuracy on coding benchmarks (e.g. HumanEval, MBPP), and licensing/availability.

## Open-Source Code Models (On-Device, Free)

These models can be run locally on your hardware (no API cost). All are free to use, though hardware requirements increase with model size. Most are transformer-based language models trained on code.

| **Model**                | **Parameters & Arch.**            | **Languages & Use Cases**                                           | **Speed / Latency**                                | **Accuracy (Benchmark)**                                                                                 | **License & Availability**                                   |
|--------------------------|-----------------------------------|---------------------------------------------------------------------|----------------------------------------------------|----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|
| **Code LLaMA (Meta)**    | 7B, 13B, 34B, 70B param transformers ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=Meta%20is%20releasing%20four%20sizes,right%20out%20of%20the%20box)) | Multi-language code assistant (Python, C++, Java, C#, PHP, JS, etc.) ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=language%20prompts%20%28e,and%20Bash)) ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=Code%20Llama%20supports%20many%20popular,and%20Bash)). Good for code completion, generation, debugging. Larger models handle more context (up to 100k tokens) ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=Code%20Llama%E2%80%99s%20Capabilities)) | Smaller 7B/13B models run faster (suitable for real-time suggestions) ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=These%20models%20cater%20to%20different,time%20code%20completion)); 34B/70B are slower (higher latency) but more accurate. | **HumanEval** pass@1: ~55% (7B), ~63% (13B), ~53.7% (34B base) ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=In%20benchmark%20tests%20using%20HumanEval,source%20solutions)); fine-tuned versions reach higher (Phind-CodeLlama 34B ~73.8% ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=Phind,tuning%20approach))). Handles long context and complex tasks better with larger models. | Meta license (open access for research & commercial use with attribution). Downloadable weights (7B–70B) ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=Meta%20is%20releasing%20four%20sizes,right%20out%20of%20the%20box)); run locally with GPU (7B can run on a single GPU ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=These%20models%20cater%20to%20different,time%20code%20completion))). |
| **StarCoder 15B**        | 15.5B param transformer ([Link](https://huggingface.co/blog/starcoder#:~:text=StarCoder%20and%20StarCoderBase%20are%20Large,model%20that%20we%20call%20StarCoder)) (+ variants: StarCoderBase, StarCoder2 at 3B/7B/15B) | Trained on The Stack (80+ languages, GitHub data) ([Link](https://huggingface.co/blog/starcoder#:~:text=StarCoder%20and%20StarCoderBase%20are%20Large,model%20that%20we%20call%20StarCoder)). Supports code completion, generation, editing (with 8K context and infill) ([Link](https://huggingface.co/blog/starcoder#:~:text=programming%20benchmarks%20and%20matches%20or,We%20take)). Good general-purpose code assistant across many languages. | ~8K context; can process large files. Generates code reasonably fast for its size (15B) – on high-end GPUs, dozens of tokens/sec. Optimized multi-query attention for faster batch inference ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=,query%20attention)). | **HumanEval** ~33–40% pass@1 for base models ([Link](https://huggingface.co/blog/starcoder#:~:text=We%20thoroughly%20evaluated%20StarCoder%20and,the%20correct%20implementation%20of%20the)) ([Link](https://huggingface.co/blog/starcoder#:~:text=Model%20HumanEval%20MBPP%20LLaMA,540B%2026.2%2036.8)). StarCoderBase outperforms older open models and even OpenAI Codex on many tasks ([Link](https://huggingface.co/blog/starcoder#:~:text=We%20found%20that%20StarCoderBase%20outperforms,autocomplete%20code%2C%20make%20modifications%20to)) ([Link](https://huggingface.co/blog/starcoder#:~:text=We%20thoroughly%20evaluated%20StarCoder%20and,the%20correct%20implementation%20of%20the)). StarCoder2 (new version) further improves performance with 3× training data. | OpenRAIL License (permissive use with attribution) ([Link](https://huggingface.co/blog/starcoder#:~:text=several%20important%20steps%20towards%20a,cases%20and%20products)). Available on HuggingFace ([Link](https://huggingface.co/blog/starcoder#:~:text=Introducing%20StarCoder)). Free to use locally (15B requires >= 16GB VRAM). |
| **PolyCoder 2.7B**       | 2.7B param GPT-2 architecture ([Link](https://accubits.com/open-source-program-synthesis-models-leaderboard/polycoder/#:~:text=match%20at%20L242%20,source%20and%20publicly%20available%2C%20enabling)) | Trained on 249 GB of code across 12 languages (C, C#, Java, Python, etc.) ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Polycoder%20is%20an%20open,Java%2C%20Python%2C%20and%20more)). Suitable for multilingual code generation; especially strong in C programming tasks. | Very fast (small model) – low latency even on CPU. Suitable for lightweight use, but may produce simpler code due to size. | Achieves **lower perplexity in C** than OpenAI Codex, *outperforming all other models in C* code generation ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Polycoder%20is%20designed%20to%20generate,other%20models%2C%20including%20OpenAI%20Codex)). Overall accuracy is lower than larger models on complex tasks, but decent given its size. | MIT License (open-source). Available for local use; 2.7B model runs on consumer GPUs or even CPUs (with reduced speed). Great for researchers or projects focusing on C ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Polycoder%20is%20designed%20to%20generate,other%20models%2C%20including%20OpenAI%20Codex)). |
| **Replit-Code v1.5-3B**  | 3B param decoder (Replit’s code model) | Supports 30 programming languages ([Link](https://www.aibase.com/news/2025#:~:text=development%20tools%2C%20integrating%20GhostWriter%20into,empowering%20developers%20around%20the%20world)). Designed for code completion and generation in the Replit IDE (functions, code translation, explanations). Good for general coding tasks with limited compute. | Real-time code completions on modest hardware. Optimized via distillation/quantization for efficiency ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=natural%20language%20queries%20and%20quickly,you%20write%20and%20refine%20code)) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=Using%20optimization%20techniques%20such%20as,you%20write%20and%20refine%20code)). Low latency for small suggestions; struggles with very large context. | Not benchmarked on public HumanEval in literature; anecdotal quality is similar to GPT-2 level. Serves as the AI “Ghostwriter” in Replit – capable of inline suggestions and code refactoring ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Replit%20Ghostwriter%20is%20an%20AI,understand%20summaries%20of%20code)) ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Replit%20Ghostwriter%20vs%20GitHub%20Copilot)). | Apache License 2.0 (open-source). Released by Replit in 2023 ([Link](https://www.aibase.com/news/2025#:~:text=Development%20tool%20provider%20Replit%20has,empowering%20developers%20around%20the%20world)). Free to use locally or within Replit; intended to democratize AI coding assistance for all users. |
| **CodeGen (Salesforce)** | 350M, 2B, 6B, 16B param GPT-style models ([Link](https://vmware.github.io/vSphere-machine-learning-extension/use-cases/codegen.html#:~:text=Salesforce%20CodeGen%20,Large%20Language%20Model%20for)) (some variants tuned on Python or multi-language) | Family of models for program synthesis ([Link](https://github.com/salesforce/CodeGen#:~:text=CodeGen%20is%20a%20family%20of,v4.%20Competitive%20with%20OpenAI%20Codex)). Can generate code from docstrings or NL prompts, translate between languages, etc. Strong in Python (mono-language version) and decent in other languages for multi-model. | Smaller variants (2B/6B) are fast; 16B model requires more compute (few seconds per function on a GPU). Supports up to 2048 tokens context. | Earlier versions achieved ~28–30% on HumanEval (16B Mono ~29.3% ([Link](https://huggingface.co/blog/starcoder#:~:text=LLaMA,7))). CodeGen2 models improved on multi-turn coding tasks. Not state-of-the-art by 2025, but useful for fine-tuning or smaller-scale needs. | Apache 2.0 license (open-source) ([Link](https://github.com/salesforce/CodeGen#:~:text=CodeGen%20is%20a%20family%20of,v4.%20Competitive%20with%20OpenAI%20Codex)). Models downloadable from HuggingFace. Free local deployment; often used as base models for further fine-tuning research. |
| **CodeGeeX 13B**         | ~13B param transformer (PanGu-Coder family) | Multilingual code assistant (supports English, Chinese, and more) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=CodeGeeX%20is%20an%20AI,for%20individual%20developers%20and%20businesses)) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=,external%20searches%20and%20enhancing%20workflow)). Capable of code generation, completion, translation, and even docstring/comment generation. Integrates with VS Code, JetBrains IDEs via extensions ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=CodeGeeX%20is%20an%20AI,for%20individual%20developers%20and%20businesses)). | Runs on a single high-end GPU. Moderate latency (a few tokens per second). Offers an open-source model for those needing a balance between size and performance. | HumanEval ~20–25% (approx, similar to CodeGen). Known to be *“close to CodeLlama 7B performance”* as of 2023. Useful for bilingual coding scenarios (e.g., commenting code in Chinese/English). | Open-source (Apache-2.0). Model available for download ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=various%20popular%20IDEs%20%20such,for%20individual%20developers%20and%20businesses)). Also provided as an **open IDE plugin**. No usage fees. |
| **WaveCoder-Ultra 6.7B** | 6.7B param transformer (Microsoft)        | General-purpose coding assistant (code generation, summarization, translation, repair) ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=The%20DeepSeek,enhance%20coding%20efficiency%20and%20accuracy)) ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=4.%20Phi)). Trained with an *enhanced instruction tuning* on diverse code tasks for strong multi-language support. | High efficiency design (uses grouped-query attention). Runs with low latency for its size. Can handle ~16k context. Suitable for on-device use where 7B is the upper limit. | **HumanEval** pass@1: ~79.9% ([Link](https://huggingface.co/microsoft/wavecoder-ultra-6.7b#:~:text=Model%20HumanEval%20MBPP%28500%29%20HumanEval%20Fix%28Avg,6.7B%2079.9%2064.6%2052.3%2045.7)) – top-tier for an open model (approaches GPT-4 performance at a fraction of size). Also 64.6% on MBPP ([Link](https://huggingface.co/microsoft/wavecoder-ultra-6.7b#:~:text=Model%20HumanEval%20MBPP%28500%29%20HumanEval%20Fix%28Avg,6.7B%2079.9%2064.6%2052.3%2045.7)). Ranked among state-of-the-art open models on code benchmarks in 2024. | MIT License ([Link](https://huggingface.co/microsoft/wavecoder-ultra-6.7b#:~:text=License)). Available on HuggingFace (released Apr 2024) ([Link](https://huggingface.co/microsoft/wavecoder-ultra-6.7b#:~:text=,2023%2F12%2F26%5D%20WaveCoder%20paper%20released)). Free to use locally; Microsoft provides code and weights openly. |
| **Qwen-2.5-Coder 7B**    | 7B param transformer (Alibaba Cloud)     | Specialized code assistant from Alibaba’s Qwen series. Supports *92 programming languages* (extremely broad coverage) and tasks from generation to debugging ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=The%20Qwen2.5,debugging%20across%20multiple%20programming%20languages)) ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=%2A%20Long,including%20Python%2C%20Java%2C%20and%20C)). Optimized through instruction tuning and massive training (5.5T tokens) for code reasoning. | Designed with long context (up to 128K tokens) ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=%2A%20Instruction%20Tuning%3A%20Fine,in%2092%20programming%20languages%2C%20including)), enabling it to work on large codebases with reasonable speed. As a 7B model, it’s relatively fast on modern GPUs, even for extended context operations (thanks to optimized context handling). | **HumanEval** score: **88.4%** pass@1 ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=The%20Qwen2.5,debugging%20across%20multiple%20programming%20languages)) ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=HumanEval%20Score%3A%2088.4)) – extraordinarily high for a 7B model, making it one of the most accurate open models (likely using advanced prompt techniques). Excels in multi-language code understanding and complex code problems. | Apache 2.0 license (open-source). Available via Alibaba Cloud open-model zoo. Free for local use and can be deployed on cloud or on-prem. Represents cutting-edge open-model performance as of early 2025. |

**Note:** *All open-source models above are free to use locally, though their licenses may require attribution or have some restrictions on commercial use. “HumanEval” is a Python benchmark; a higher pass@1 percentage means the model can solve more coding problems correctly on the first try. Benchmarks for other languages (e.g. MultiPL-E) may vary. In general, larger models (or fine-tuned versions) achieve higher accuracy but run slower.* ([Link](https://medium.com/@marketing_novita.ai/introducing-code-llama-a-state-of-the-art-large-language-model-for-code-generation-e9753deb61b7#:~:text=These%20models%20cater%20to%20different,time%20code%20completion)) ([Link](https://huggingface.co/blog/starcoder#:~:text=We%20thoroughly%20evaluated%20StarCoder%20and,the%20correct%20implementation%20of%20the))

## Proprietary Cloud-Based Code Models

These models are offered as services (via API or integrated tools). They typically have usage-based pricing or subscriptions. They often leverage massive model sizes (not publicly disclosed) and benefit from proprietary data. Higher-priced tiers usually unlock larger or faster models and greater usage limits.

| **Model**             | **Pricing (API/Subscription)**                                             | **Languages & Use Cases**                                             | **Model & Architecture**                              | **Speed / Latency**                                          | **Accuracy & Benchmarks**                                                                                 | **Availability & License**                                                |
|-----------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------|--------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| **OpenAI GPT-4**      | *API:* ~$0.03 per 1K prompt tokens, $0.06 per 1K output tokens (8K context) ([Link](https://www.ibbaka.com/ibbaka-market-blog/how-openai-prices-the-inputs-and-the-outputs-for-gpt4#:~:text=Input%3A%20%240,per%20thousand%20tokens)); 32K context costs double ($0.06/$0.12 per 1K) ([Link](https://www.ibbaka.com/ibbaka-market-blog/how-openai-prices-the-inputs-and-the-outputs-for-gpt4#:~:text=Input%3A%20%240,per%20thousand%20tokens)). *ChatGPT Plus:* $20/month (includes GPT-4 access in chat). Higher context or priority access via **ChatGPT Enterprise** (negotiated pricing). | Over a dozen languages (strongest in Python, JS, Java, C++ etc.). Excels at complex coding tasks, algorithmic problems, and debugging. Used for code generation, review, and natural language Q&A about code. Suitable for **advanced use cases** (e.g. writing entire modules, tricky debugging). |  Undisclosed transformer-based LLM (estimated >170B parameters). Trained on a broad mix of code and text. State-of-the-art reasoning and coding abilities with vision and tool-use support in newer versions ([Link](https://openai.com/api/pricing/#:~:text=Reasoning%20models%20for%20complex%2C%20multi,problems)) ([Link](https://openai.com/api/pricing/#:~:text=OpenAI%20o3)). | Slower than smaller models – rate limited (~15-32 tokens/second in practice). High inference cost means API responses have noticeable latency for long outputs. Optimized “turbo” versions and system upgrades in late 2024 improved speed and cost ([Link](https://www.reddit.com/r/mlscaling/comments/17pkem7/what_do_we_learn_from_the_gpt4_price_drop/#:~:text=What%20do%20we%20learn%20from,01%2F1k%29%20and%202X)). | **HumanEval** ~85-88% pass@1 (cutting-edge) ([Link](https://arxiv.org/html/2402.14852v1#:~:text=GPT,TABLE%20I%3A%20Model%20performance%20comparison)). Essentially at or near human-level on many programming challenges. Wins or performs top-tier on coding benchmarks (MBPP, LeetCode, etc.). Consistently outperforms most other models in code correctness ([Link](https://arxiv.org/html/2402.14852v1#:~:text=%23%20%20II,for%20Models%20to%20Pass%20HumanEval)) ([Link](https://arxiv.org/html/2402.14852v1#:~:text=GPT,TABLE%20I%3A%20Model%20performance%20comparison)). | Commercial (proprietary). Available via OpenAI API and Azure OpenAI. No on-premise. **License:** Usage governed by OpenAI terms (no IP rights to model). |
| **OpenAI GPT-3.5 Turbo** | *API:* $0.0005 per 1K input tokens + $0.0015 per 1K output tokens ([Link](https://community.openai.com/t/pricing-of-legacy-models/614008#:~:text=%2A%20gpt)) (16K context version slightly more). *Azure and third-party services* may have different pricing. Included in ChatGPT free tier (with rate limits). | Dozens of languages (very strong in Python, JS; good in others). Ideal for day-to-day code completion, generating boilerplate, and answering coding questions. Powers many coding assistants with fast responses. Use cases: autocompletion, code explanation, simple script generation. | ~Turbo GPT-3.5 model (instruct-tuned GPT-3.5 series). Around 175B parameters (not officially stated). Fine-tuned on code and instructions. Lacks some of GPT-4’s advanced reasoning but much faster and cheaper. | Very fast generation (~50-70 tokens/sec in short bursts). Low latency for small-to-medium outputs. Can handle real-time IDE integration. However, quality drops on very complex tasks compared to GPT-4. | **HumanEval** ~48-55% (older version ~48% ([Link](https://openai.com/index/gpt-4-research/#:~:text=GPT,Reading)); updated 16k-context versions improved further). Strong performance on MBPP (solves many basic Python problems). Good for routine tasks but will miss more complex problem logic that GPT-4 or Claude can handle. | Commercial API. No local version. License restricted to API terms. Widely integrated (e.g. GitHub Copilot’s earlier versions, Azure OpenAI, etc.). Very cost-effective for coding assistance given its low price/token. |
| **GitHub Copilot**    | *Subscription:* **$10/month** (or $100/year) for individuals ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=)); free for students and maintainers. **Business** plan at $19/user/month ([Link](https://aihungry.com/tools/amazon-codewhisperer/pricing#:~:text=2025%20aihungry,Administrators%20get%20organizational%20license)) ([Link](https://aws.amazon.com/about-aws/whats-new/2023/04/amazon-codewhisperer-generally-available/#:~:text=Amazon%20CodeWhisperer%20is%20now%20generally,capabilities%20to%20organizations%20that)). *Copilot Chat* (with GPT-4/Claude) included for subscribers. | Supports all major languages (Python, JavaScript/TypeScript, Go, Ruby, Java, C#, C/C++, SQL, etc.). Best for in-IDE code completion, suggesting the next line or block as you type. Also provides **Copilot Chat** for explaining code and answering questions in natural language. Use cases: speeding up coding by autocompleting snippets and writing tests/docs. | Powered by OpenAI models (Codex initially; now GPT-4 and others). As of 2024, Copilot uses **GPT-4** for interactive chat and a mix of GPT-4 or optimized 3.5 models for realtime completions ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=GitHub%20Copilot%20is%20an%20AI,5%20Sonnet%2C%20and%20integrates%20with)) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=GitHub%20Copilot%20offers%20a%20Free,starting%20at%20%2410%20per%20month)). Also integrated **Anthropic Claude 3.5** for some features ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=GitHub%20Copilot%20is%20an%20AI,5%20Sonnet%2C%20and%20integrates%20with)). Copilot is an AI *service* rather than a single model. | Real-time suggestions with low latency (a few hundred ms for line completions). Chat responses (GPT-4) take a few seconds. GitHub optimizes requests to reduce lag in the IDE. | No public benchmark, but OpenAI’s Codex (which Copilot started with) solved ~37% of HumanEval in 2021. Copilot’s quality improved with GPT-4 backend – roughly matches GPT-4’s accuracy for multi-step problems in Chat mode. In practice, **Copilot autocompletes ~50-60% of code correctly** in supported frameworks (per GitHub’s own metrics). | Commercial. **Availability:** VS Code, Visual Studio, JetBrains, Neovim, etc. via extension ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=coding%20tasks%2C%20providing%20autocomplete,directly%20within%20the%20IDE)). Requires cloud (GitHub’s service). **License:** Proprietary SaaS; generated code may contain fragments from training data (GitHub provides legal indemnification for users). |
| **Amazon CodeWhisperer** | **Free for individual use** (unlimited, requires AWS login) ([Link](https://docs.aws.amazon.com/codewhisperer/latest/userguide/billing.html#:~:text=This%20page%20describes%20the%20different,and%20easy%20to%20set%20up)) ([Link](https://www.g2.com/products/amazon-codewhisperer/reviews#:~:text=Amazon%20CodeWhisperer%20Reviews%202025%3A%20Details%2C,and%20there%20are%20no)). **Professional** $19/user/month for organizations ([Link](https://aihungry.com/tools/amazon-codewhisperer/pricing#:~:text=2025%20aihungry,Administrators%20get%20organizational%20license)) ([Link](https://aws.amazon.com/about-aws/whats-new/2023/04/amazon-codewhisperer-generally-available/#:~:text=Amazon%20CodeWhisperer%20is%20now%20generally,capabilities%20to%20organizations%20that)). | Multi-language support (Python, Java, JavaScript, TypeScript, C#, Go, Rust, SQL, etc. – focused on popular languages). Use cases: similar to Copilot – inline code suggestions in IDEs, especially for AWS-related development. Excels at code completion and creating AWS APIs snippets. Also includes security scans for vulnerabilities. | Underlying model details not public (AWS-trained transformer ~15B parameters rumored). Optimized for cloud integration and AWS APIs. Tight IDE integration (VS Code, JetBrains). Slightly less “chatty” than GPT-based models – more focused on completing code given preceding context. | Fast suggestions (aims for sub-second latency in IDE). AWS infrastructure ensures low latency for users in supported regions. For larger comment-to-code generation, takes a second or two. | In AWS’s internal benchmarks, CodeWhisperer was shown to produce correct suggestions **57%** of the time on common tasks vs 27% for Copilot (pre-GPT4) ([Link](https://hackr.io/blog/github-copilot-vs-amazon-codewhisperer#:~:text=GitHub%20Copilot%20vs%20Amazon%20CodeWhisperer,Individual%20plan%20is%20free%2C)) (Amazon’s claims). On HumanEval, independent tests found performance comparable to Codex (around 30-40% pass@1). Strengths in AWS-centric code patterns. | Commercial service. **Availability:** AWS Toolkit plugins for IDEs (VS Code, IntelliJ, PyCharm, etc.). No local version. Free tier for individuals (no license needed), pro tier requires AWS account. |
| **Amazon Q Developer** | **Free tier** available; **Paid Pro** plan *starting at* **$19/month per user** ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=Amazon%20Q%20Developer%20is%20a,repetitive%20tasks%2C%20and%20assists%20with)) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=)). (Pricing covers advanced capabilities like code base customization and higher usage limits.) | Generative AI assistant for full software development lifecycle on AWS ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=Amazon%20Q%20Developer%20is%20a,based%20applications%20and%20infrastructure)). Supports code in multiple IDEs (JetBrains, VS Code, Visual Studio) with **code generation, review, refactoring, testing** help ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=Amazon%20Q%20Developer%20is%20a,based%20applications%20and%20infrastructure)) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=,Customizes%20responses%20based%20on%20internal)). Use cases: project onboarding (answer questions about a codebase), converting natural language requests into code (feature generation), cloud infrastructure code (IaC) and AWS service integration. | Combination of models (likely uses a version of CodeWhisperer plus larger Alexa Teacher Model variants). Emphasizes a *conversational agent* approach. Integrates with AWS services for context (cloud architecture diagrams, etc.). The model(s) behind Q are not fully disclosed; presumably large Transformers hosted on AWS. | Low latency for inline code completion. More involved tasks (like multi-step feature generation or answering questions about a project) take a few seconds as Q may run more complex prompts. Aims to maintain interactive speeds for conversation-style assistance. | Not publicly benchmarked on standard code tests yet (new in late 2024). Amazon claims “state-of-the-art” capabilities in code generation and transformation. In internal evals, it can complete code tasks and even execute and test code in realtime (for verification) ([Link](https://aws.amazon.com/blogs/devops/enhancing-code-generation-with-real-time-execution-in-amazon-q-developer/#:~:text=Enhancing%20Code%20Generation%20with%20Real,test%20code%20in%20real%20time)) ([Link](https://aws.amazon.com/awstv/watch/ee812df95bc/#:~:text=Refactor%20Code%20with%20Amazon%20Q,refactoring%20within%20your%20favorite%20IDE)). Likely on par with other GPT-4-class offerings for supported tasks, given it leverages advanced AWS AI. | Commercial (AWS cloud service). **Availability:** AWS IDE extensions and AWS Console (no local/offline mode). Licensing through AWS (terms of service). Geared toward enterprise use on AWS – integrates with AWS Code Suite and cloud resources. |
| **Anthropic Claude**   | *API (Claude 2/Claude 3.5):* **Claude Instant** ~$0.0024/1K output tokens ([Link](https://latenode.com/blog/claude-ai-pricing-and-features#:~:text=Claude%20Instant)); **Claude 2 / 3.5** ~$0.015/1K output tokens ([Link](https://latenode.com/blog/claude-ai-pricing-and-features#:~:text=Claude%203%20Sonnet)) ([Link](https://latenode.com/blog/claude-ai-pricing-and-features#:~:text=Claude%203)) (with input tokens one-fifth that cost) – i.e. $15 per million output tokens for top model. *Claude Pro (web)*: $20/month for unlimited usage on Claude.ai ([Link](https://www.datacamp.com/blog/claude-sonnet-anthropic#:~:text=Image%3A%20Claude%20AI%20graphical%20user,interface)). | Very strong in English-centric coding (Python, JS, Java, etc. with excellent reasoning). Also handles other languages (less training on niche languages than CodeLlama). Great at **conversational coding**: explaining code, reviewing for errors, and writing functions given high-level instructions. Up to 100K context window (reads very large code files). Use cases: interactive pair-programmer, code analysis, generating test cases, etc. | Claude 2 and Claude 3.5 are proprietary large transformers (50B+ params, details not public). Trained with “Constitutional AI” safety. Known for long-context handling (100k tokens) and fast reasoning. Claude Instant is a smaller, faster model (~<10B params) for lightweight tasks. | Faster than GPT-4 in many cases. **Claude Instant** is extremely fast (on par with GPT-3.5 Turbo) for short prompts. Claude 2/3 have slightly lower throughput than GPT-3.5 but better than GPT-4. With 100k context, latency grows if you actually feed huge inputs. Generally a few seconds for typical function-generation prompts. | Claude 2 achieved ~71% on HumanEval (Python) in mid-2023. **Claude 3.5 “Sonnet”** improved coding performance significantly – it’s now outperforming GPT-4 on some code benchmarks (reportedly ~98% pass@10 on HumanEval) ([Link](https://paperswithcode.com/sota/code-generation-on-humaneval#:~:text=HumanEval%20Benchmark%20%28Code%20Generation%29%20,4o%29.%2098.2)) ([Link](https://news.ycombinator.com/item?id=37268146#:~:text=GPT,was)). Extremely good at reasoning through multi-step problems and generating correct solutions, often rivaling GPT-4 in quality. | Commercial. **Availability:** Anthropic API, also integrated in platforms like Slack, Sourcegraph Cody, GitHub Copilot (as of 2024) ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=GitHub%20Copilot%20is%20an%20AI,5%20Sonnet%2C%20and%20integrates%20with)). Also on AWS Bedrock and Google Cloud Vertex AI ([Link](https://www.datacamp.com/blog/claude-sonnet-anthropic#:~:text=Claude%203,Vertex%20AI%2C%20and%20Amazon%20Bedrock)). License is proprietary (with emphasis on safety and usage guidelines). No self-host option. |
| **Google Codey (PaLM 2 Code)** | *Vertex AI API:* ~$0.00025 per 1K input chars, $0.0005 per 1K output chars ([Link](https://cloud.google.com/vertex-ai/generative-ai/pricing#:~:text=Note%3A%20Prediction%20pricing%20for%20tuned,for%20Code%20Completion%20Input%20Global)) ([Link](https://cloud.google.com/vertex-ai/generative-ai/pricing#:~:text=Codey%20for%20Code%20Completion%20Input,Global)) (during preview; roughly $0.00075/1K total). Google **Duet AI** (which uses Codey) is included for Google Cloud subscribers (pricing baked into GCP costs). | Supports over 20 languages (Python, Java, JavaScript, Go, Kotlin, etc.) ([Link](https://voicebot.ai/2023/05/19/google-rolls-out-codey-ai-coding-assistant/#:~:text=The%20new%20Codey%20family%20of,useful%20in%20scientific%20research%20and)) ([Link](https://voicebot.ai/2023/05/19/google-rolls-out-codey-ai-coding-assistant/#:~:text=%E2%80%9CCodey%20was%20fine,%E2%80%9D)). Use cases: code completion in Google Colab and Cloud IDEs, code chat assistance (explaining and fixing code), and generating Cloud SDK code. Specialized in assisting with **Google frameworks/APIs** as well. Strong focus on Python (Colab) use. | PaLM 2 large language model, fine-tuned on high-quality code (Google’s internal and external permissive code) ([Link](https://voicebot.ai/2023/05/19/google-rolls-out-codey-ai-coding-assistant/#:~:text=%E2%80%9CCodey%20was%20fine,%E2%80%9D)). Variants: Codey  code-completion model, and Codey chat model. Exact size not disclosed (PaLM 2 has sizes up to 540B; Codey likely based on a 64B or smaller variant for speed). | Generally fast API responses. In Google Colab, Codey can suggest completions almost instantly for small prompts. For multi-sentence prompts via API, responds in under a second typically (given its pricing is very low per character, it’s optimized for speed/cost). | Google hasn’t published HumanEval, but Codey is reported to lag behind GPT-4/Claude, roughly comparable to GPT-3.5 level. In one benchmark, **Codey (PaLM2)** was around 30–40% on HumanEval. CodeLlama 34B outperformed Codey in public tests ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=In%20benchmark%20tests%20using%20HumanEval,source%20solutions)). Still, Codey is competent on common tasks and steadily improving, especially for Python (customized for Colab) ([Link](https://voicebot.ai/2023/05/19/google-rolls-out-codey-ai-coding-assistant/#:~:text=%E2%80%9CCodey%20was%20fine,%E2%80%9D)). | Proprietary. **Availability:** Google Cloud Vertex AI (API), Google Colab & Cloud Shell (as “assistant”). Not available for on-premise. Licensing through Google Cloud terms. Duet AI (with Codey) is offered to enterprise Google Workspace/Cloud customers as well. |
| **Tabnine**            | **Basic:** Free (local ML model with limited capability) ([Link](https://www.wheelhouse.com/products/tabnine/pricing#:~:text=Tabnine%20offers%20three%20pricing%20plans%3A)). **Pro (Dev):** $9/user/month ([Link](https://www.wheelhouse.com/products/tabnine/pricing#:~:text=,39%20per%20user%20per%20month)). **Enterprise:** $39/user/month ([Link](https://www.wheelhouse.com/products/tabnine/pricing#:~:text=Tabnine%20offers%20three%20pricing%20plans%3A)) (advanced features, self-host option) – pricing includes unlimited code completion usage. | Supports **all major languages** (and many minor ones). Focused on **contextual code completion** rather than long dialogs – it learns from your code context to suggest next lines or blocks. Good for accelerating typing of boilerplate, common idioms, and repetitive code. Enterprise version can train on your *own codebase* for personalized suggestions ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Tabnine%20stands%20out%20for%20its,the%20team%E2%80%99s%20practices%20and%20guidelines)) ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=coding%20suggestions%20and%20is%20integrated,specific%20coding%20assistance)). | Uses a proprietary code-specific model (original Tabnine was GPT-2 based; newer versions incorporate open models like Codegen with custom tuning). The **local model** (for offline use) is lightweight, while the cloud model used for Pro is larger and more powerful. Integrates into editors to fetch cloud completions. | Very low latency for single-line suggestions (designed for real-time IDE usage). The cloud inference for whole-function suggestions is also quick (often <1s). The local offline model is instant but less accurate; cloud boosts quality at slight cost in latency. | Not directly benchmarked on public leaderboards. Tabnine’s quality is roughly on par with Codex on common library usage. It won’t solve hard algorithmic puzzles, but it excels at **completing structured code** (like filling in function arguments, repetitive code patterns). In enterprise settings, fine-tuned on your code, it can produce *up to 50% of code* according to user reports. | **Availability:** Plugins for all popular IDEs. Basic plan runs fully local (no data leaves your machine), Pro/Enterprise send code context to Tabnine’s cloud. **License:** Proprietary software. Enterprises can opt for self-hosted deployment to keep data on-prem. |
| **Replit Ghostwriter** | **Free tier:** included for all Replit users with basic features as of 2023 ([Link](https://www.aibase.com/news/2025#:~:text=Development%20tool%20provider%20Replit%20has,empowering%20developers%20around%20the%20world)). **Ghostwriter Pro:** $20/month (previously $10, includes more AI credits and faster generation). | Supports dozens of languages on Replit.com (including web dev, Python, C/C++, Java, Ruby, etc.). Use cases: inline code completion, a chat-based **AI helper**, code transformation (e.g. “make this code more readable”), and explanation of code snippets ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Replit%20Ghostwriter%20is%20an%20AI,understand%20summaries%20of%20code)) ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=Replit%20Ghostwriter%20vs%20GitHub%20Copilot)). Particularly useful for learners and quick scripting. | Combines models: uses OpenAI GPT-4 for some features ([Link](https://davidmelamed.com/2025/02/18/the-best-ai-coding-tools-according-to-chatgpts-deep-research/#:~:text=The%20Best%20AI%20Coding%20Tools,code)), and Replit’s own 3B model for others. Replit is actively developing larger proprietary models. Ghostwriter’s architecture is a hybrid system directing prompts either to their in-house model or external APIs depending on task complexity. | In-browser performance is snappy for completions (small model). Full-function generations or chat (which may invoke GPT-4) have a few seconds delay. Replit streams output when generating larger code blocks, improving perceived latency. | Comparable to GPT-3.5 on many tasks when using their own model. For difficult problems, it falls back to GPT-4 (hence achieving high quality at times). On HumanEval, the Replit-code 3B model alone would be modest (~20-30%), but with GPT-4 assist, Ghostwriter solves most prompts. It particularly shines in *code repair and explanation* tasks, aided by Replit’s integration (can actually run the code to diagnose issues). | Proprietary service by Replit. Only available within Replit’s online IDE. No standalone model download (aside from the open 3B model). **License:** Code generated is owned by the user (per Replit terms), but the service itself is closed-source. |
| **Sourcegraph Cody**   | **Free:** limited usage with open-source code context. **Pro:** $10/user/month (increased limits, private repositories) – as of 2025. **Enterprise:** $59/user/month ([Link](https://www.g2.com/products/sourcegraph-sourcegraph-cody/pricing#:~:text=Sourcegraph%20Cody%20Pricing%202025%20Sourcegraph,at%20different%20pricing%20editions%20below)) with self-host options and unlimited use. | Supports dozens of languages. Integrated into Sourcegraph’s code search platform. Use cases: answering questions about a large codebase, doing refractors across repos, explaining code, and generating code with full repository context. Essentially, Cody is like a “coding search + GPT” that can index your entire org’s code and answer questions or generate code in that context. | Cody is an *AI-powered layer* over Sourcegraph’s search. It uses large models (Anthropic Claude by default, and others like GPT-4 optionally) to perform the tasks ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=GitHub%20Copilot%20is%20an%20AI,5%20Sonnet%2C%20and%20integrates%20with)). The architecture involves a **context window** stuffed with relevant code retrieved by Sourcegraph, then a prompt to the AI model. The heavy lifting is done by Claude or GPT under the hood (no custom model from Sourcegraph itself, though they orchestrate it). | Latency depends on the underlying model (Claude is fast for Q&A; GPT-4 a bit slower). For code search queries, Cody first fetches relevant code snippets (fast) then the model generates answers (1-3 seconds typically). Designed to handle fairly large prompts (since it can feed in entire files as needed). | With Claude-2, Cody can achieve very high accuracy in code Q&A – e.g. finding a function usage across a codebase or summarizing a file’s purpose correctly. On HumanEval-style generation, its performance is that of the model it uses (Claude or GPT-4, i.e. ~80%+). Cody’s strength is less in standalone generation and more in *code understanding* with context: it can integrate 100k+ tokens of code context, giving relevant answers that simpler assistants cannot. | **Availability:** as a cloud service on sourcegraph.com, or self-hosted Sourcegraph Enterprise (you provide API keys for the LLM). The “Cody” interface itself is open-source ([Link](https://news.ycombinator.com/item?id=35339010#:~:text=Open%20Sourcing%20Cody%20%E2%80%93%20Sourcegraph%27s,to%20improve)), but the models are not – it relies on third-party APIs. **License:** Code answers are derived from your code (so under your code’s license); the service itself is commercial. |

**Notes:** Higher-cost plans for cloud models often unlock larger context windows or more powerful versions. For example, GitHub Copilot for Business and Amazon CodeWhisperer Professional offer admin controls and improved suggestion relevance. Similarly, ChatGPT Enterprise gives prioritized GPT-4 access with longer context. *When evaluating cost, consider that some providers charge per token (OpenAI, Anthropic, Google), while others are flat monthly fees (Copilot, Tabnine, etc.).* The **accuracy** of models is continually improving – by 2025 even the open models (e.g. WizardCoder, Phind-CodeLlama) have closed the gap with proprietary ones on benchmarks ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=Phind%2C%20an%20AI%20company%2C%20has,pass%401%2C%20respectively)) ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=Phind,tuning%20approach)), but top-tier closed models (GPT-4, Claude) still lead on toughest problems.

All on-device models listed are **free** (open-source), whereas cloud models incur costs either per token or per seat. For critical applications requiring maximum accuracy and less concern for cost, services like GPT-4 or Claude are preferred. For cost-sensitive or privacy-sensitive scenarios, open-source models (Code LLaMA, StarCoder, etc.) running locally or in self-hosted setups can be a viable alternative, albeit with some trade-offs in raw performance. Each model above has unique strengths, so the best choice depends on your use case, budget, and deployment constraints. 

**Sources:** Benchmark scores and model details are cited from relevant literature and documentation for each model ([Link](https://www.scribbledata.io/blog/the-top-llms-for-code-generation-2024-edition/#:~:text=development%2C%20machine%20learning%2C%20and%20natural,other%20models%2C%20including%20OpenAI%20Codex)) ([Link](https://huggingface.co/microsoft/wavecoder-ultra-6.7b#:~:text=Model%20HumanEval%20MBPP%28500%29%20HumanEval%20Fix%28Avg,6.7B%2079.9%2064.6%2052.3%2045.7)) ([Link](https://hub.athina.ai/blogs/top-open-source-models-for-code-generation-in-2025/#:~:text=The%20Qwen2.5,debugging%20across%20multiple%20programming%20languages)) ([Link](https://www.e2enetworks.com/blog/top-8-open-source-llms-for-coding#:~:text=Phind%2C%20an%20AI%20company%2C%20has,pass%401%2C%20respectively)), and pricing information is taken from official pricing pages or announcements as of March 2025 ([Link](https://zencoder.ai/blog/generative-ai-code-generation-tools#:~:text=)) ([Link](https://latenode.com/blog/claude-ai-pricing-and-features#:~:text=Claude%203%20Sonnet)) ([Link](https://cloud.google.com/vertex-ai/generative-ai/pricing#:~:text=Note%3A%20Prediction%20pricing%20for%20tuned,for%20Code%20Completion%20Input%20Global)).
